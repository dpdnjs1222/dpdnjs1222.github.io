1. 현재 우리쪽 동향(gpt3, bert-step)
https://hwiyong.tistory.com/392
2. 내가 어떤 실험들 했는지
3. LM 기법들
4. tensorflow(bert 코드), pytorch 차이점, horovod, fp16, xla
https://y-rok.github.io/deep%20learning/2019/12/19/horovod-tensorflow.html
5. POI가 뭔지
6. rnn, lstm, bert, gan, cnn
lstm : RNN의 히든 state에 cell-state를 추가한 구조
state가 꽤 오래 경과하더라도 그래디언트가 비교적 전파가 잘 되게 됩니다
cnn : https://gruuuuu.github.io/machine-learning/cnn-doc/#
7. 특허
8. semi supervised
10. 동일님이 쓴 논문, 내가 쓴 논문

tokenization
https://lovit.github.io/nlp/2018/04/02/wpm/
많이 쓰이는 subwords 를 units 으로 이용하면, 자주 이용되는 단어는 그 자체가 unit 이 되며, 자주 등장하지 않는 단어 (rare words)가 subword units 으로 나뉘어집니다.

9. embedding, word2vec(skipgram)
An embedding is a mapping of a discrete — categorical — variable to a vector of continuous numbers. In the context of neural networks, embeddings are low-dimensional, learned continuous vector representations of discrete variables
비슷한 의미의 단어들은 비슷한 벡터로 표현이 된다면? 더 나아가 단어와 단어 간의 관계가 벡터를 통해서 드러날 수 있다면?

대상의 속성을 표현하는 방식 feature representation

Dense representation 하나의 차원이 여러 속성들이 버무려진 정보를 들고 있다
https://dreamgonfly.github.io/blog/word2vec-explained/


- collaborative filtering : 나와 비슷한 유저
collaborative filtering is a method of making automatic predictions (filtering) about the interests of a user by collecting preferences or taste information from many users (collaborating). The underlying assumption of the collaborative filtering approach is that if a person A has the same opinion as a person B on an issue, A is more likely to have B's opinion on a different issue than that of a randomly chosen person.
poi-user : 클릭수를 관심으로 간주
- item embedding : item co-occurrence matrix factorization
poi-poi : 같이 나오는 POI는 서로 유사


Q) 시간, 사용자 선호 분위기