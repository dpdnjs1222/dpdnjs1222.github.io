<!DOCTYPE html>
<html lang="kor">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"dpdnjs1222.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="article">
<meta property="og:title" content="[논문공부] DeBERTa: Decoding-enhanced BERT with Disentangled Attention">
<meta property="og:url" content="https://dpdnjs1222.github.io/2021/02/12/2021-02-12-deberta/index.html">
<meta property="og:site_name" content="방예의 블로그">
<meta property="og:locale">
<meta property="og:image" content="https://user-images.githubusercontent.com/16619941/107765448-36079580-6d75-11eb-9d26-f1ac589edb17.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/16619941/107765770-be863600-6d75-11eb-9f95-10a0faf71ed2.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/16619941/107741049-c206c680-6d4f-11eb-940b-521fc1fddf75.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/16619941/107786630-796ffd00-6d91-11eb-87ba-9725cabd99cd.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/16619941/107786641-7c6aed80-6d91-11eb-8846-d5789b04104c.png">
<meta property="og:image" content="https://user-images.githubusercontent.com/16619941/107786653-7ffe7480-6d91-11eb-8df6-1342da38da15.png">
<meta property="article:published_time" content="2021-02-11T15:00:00.000Z">
<meta property="article:modified_time" content="2021-02-14T11:14:51.686Z">
<meta property="article:author" content="Yewon Park">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://user-images.githubusercontent.com/16619941/107765448-36079580-6d75-11eb-9d26-f1ac589edb17.png">

<link rel="canonical" href="https://dpdnjs1222.github.io/2021/02/12/2021-02-12-deberta/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'kor'
  };
</script>

  <title>[논문공부] DeBERTa: Decoding-enhanced BERT with Disentangled Attention | 방예의 블로그</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">방예의 블로그</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="kor">
    <link itemprop="mainEntityOfPage" href="https://dpdnjs1222.github.io/2021/02/12/2021-02-12-deberta/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yewon Park">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="방예의 블로그">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          [논문공부] DeBERTa: Decoding-enhanced BERT with Disentangled Attention
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2021-02-12 00:00:00" itemprop="dateCreated datePublished" datetime="2021-02-12T00:00:00+09:00">2021-02-12</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2021-02-14 20:14:51" itemprop="dateModified" datetime="2021-02-14T20:14:51+09:00">2021-02-14</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p><escape><a id="more"></a></escape><br>Microsoft에서 나온 언어 모델이며 현재 1.5B 학습 파라미터를 사용하여 human performance를 넘었고 SOTA를 찍은 모델이다.(1/6, 2021)<br><img src="https://user-images.githubusercontent.com/16619941/107765448-36079580-6d75-11eb-9d26-f1ac589edb17.png" alt="superglue"></p>
<p>2가지 핵심이 되는 기법(disentangled attention, enhanced mask decoder)이 추가되고 finetuning에서 regularization이 추가되었다. 이에 대한 설명은 microsoft blog에서 확인이 가능하다.<br><a target="_blank" rel="noopener" href="https://www.microsoft.com/en-us/research/blog/microsoft-deberta-surpasses-human-performance-on-the-superglue-benchmark/">microsoft blog DeBERTa 설명</a></p>
<h2 id="구조"><a href="#구조" class="headerlink" title="구조"></a>구조</h2><p><img src="https://user-images.githubusercontent.com/16619941/107765770-be863600-6d75-11eb-9f95-10a0faf71ed2.png" alt="deberta 구조">  </p>
<h3 id="1-Disentangled1-attention"><a href="#1-Disentangled1-attention" class="headerlink" title="1. Disentangled1 attention"></a>1. Disentangled<sup id="a1"><a href="#footnote1">1</a></sup> attention</h3><p>self-attention은 단어의 위치 정보를 인코딩하지 못하기 때문에 기존 모델들은 단어 embedding에 positional bias를 추가하였다. positional bias는 absolute position embedding<sup id="a1"><a href="#footnote1">2</a></sup>이나 relative position embedding<sup id="a1"><a href="#footnote1">3</a></sup>을 이용할 수 있다. 이전에 relative position embedding이 NLU와 NLG에 더 효과적이라는 연구들이 있었다.<sup id="a1"><a href="#footnote1">4</a></sup>  </p>
<p>단어 “deep”과 “learning”이 있다고 해보자. 이 두 단어가 연달아서 나오는 경우와 각각 다른 문장에서 나오는 경우 중 “deep”과 “learning” 단어의 관련도는 전자가 후자보다 훨씬 클 것이다. 문장에서 토큰 사이의 관련도를 나타내는 attention weight는 단어의 의미 뿐 아니라 상대적 위치에도 영향을 받는다.<br>BERT와 같은 기존 모델은 하나의 벡터로 문장의 토큰을 표현한다. 이 토큰을 나타내는 벡터는 word(content) embedding과 position embedding의 합이다.<br><strong>하지만 DeBERTa는 두가지 벡터(content vector와 relative position vector)로 문장의 토큰을 표현한다. 또한, 토큰 사이의 attention weight는 content와 relative position에 기반한 disentangled matrices를 이용하여 계산된다.</strong>   </p>
<p><img src="https://user-images.githubusercontent.com/16619941/107741049-c206c680-6d4f-11eb-940b-521fc1fddf75.png" alt="disentangled_attention"><br>위 수식에서 i 위치의 토큰은 Hi(content vector), Pi|j(j 위치의 토큰과의 relative position vector) 두가지로 표현되었다. 위 수식은 i 위치와 j 위치의 토큰 사이의 attention score을 나타낸다. i 위치와 j 위치 토큰 사이의 cross attention score을 구하기 위해 네개의 attention score로 분리하였다. 각각의 attention score은 앞에서부터 content-to-content, content-to-position, position-to-content, position-to-position을 의미한다.<br>기존 relative position encoding 접근 방법으로는 content-to-content, content-to-position밖에 고려하지 못한다.<sup id="a1"><a href="#footnote1">5</a></sup> 하지만 attention weight는 content 뿐 아니라 relative position의 영향을 받기 때문에 position-to-content도 중요하다.<br>또한, 현재 relative position embedding을 사용하고 있기 때문에 position-to-position은 전체 attention score에 큰 영향을 미치지 않는다. 그래서 이 부분은 수식에서 제외된다.  </p>
<p>위에서 설명한 바를 요약하자면 disentangled attention 계산을 위해 content-to-content, content-to-position, position-to-content score로 분해한다. 이제 relative position이 적용된 disentangled self-attention의 수식을 살펴보자.<br><img src="https://user-images.githubusercontent.com/16619941/107786630-796ffd00-6d91-11eb-87ba-9725cabd99cd.png" alt="deberta3"><br>기존 self-attention처럼 query, key, value를 구하기 위해 projection matrix(W)를 곱한다. P는 relative position embedding을 나타낸다. Qc, Kc, Vc는 content vector를 나타내고 Qr, Kr은 relative position vector를 나타낸다.</p>
<p><img src="https://user-images.githubusercontent.com/16619941/107786641-7c6aed80-6d91-11eb-8846-d5789b04104c.png" alt="deberta4"><br>토큰 i와 j의 attention score를 나타내는 식이다.<br>(a) Qc의 i번째 열과 Kc의 j번째 열의 곱이다.<br>(b) Qc의 i번째 열과 Kr의 delta(i, j)번째 열의 곱이다. (여기서 delta(i, j)는 i 토큰과 j 토큰의 상대적 거리이다)<br>(c) Kc의 j번째 열과 Qr의 delta(j, i)번째 열의 곱이다.  </p>
<p><img src="https://user-images.githubusercontent.com/16619941/107786653-7ffe7480-6d91-11eb-8df6-1342da38da15.png" alt="deberta5"></p>
<p>disentagled attention 알고리즘은 DeBERTa 논문에서 확인할 수 있다. 논문에서 i번째 토큰과 j번째 토큰의 상대적 거리를 [0, 2k)로 한정하였고 모든 relative position embedding들은 Kr의 부분집합이기 때문에 모든 query에 대해 attention을 구할 때 Kr을 재사용할 수 있다. 이 때문에 relative position embedding 저장 공간 복잡도는 O(N<em>N</em>d)가 아니라 O(kd)이다.  </p>
<h3 id="2-Enhanced-masked-decoder-EMD"><a href="#2-Enhanced-masked-decoder-EMD" class="headerlink" title="2. Enhanced masked decoder(EMD)"></a>2. Enhanced masked decoder(EMD)</h3><p>DeBERTa는 BERT와 마찬가지로 MLM(masked language model)으로 학습된다. MLM은 마스크 토큰을 예측하기 위해 주변 단어를 사용하기 때문에, Deberta는 위에서 설명한 disentangled attention를 통해 마스크 토큰을 예측하기 위해 주변 단어들의 content와 relative position 정보를 고려할 것이다. 하지만 absolute position 정보는 고려하지 못한다.<br>하지만, 예를 들어 “a new store opened beside the new mall” 이라는 문장에서 “store”과 “mall”을 예측해야 한다면 “store”과 “mall”을 구분하기 위해 content와 relative position 정보만 고려하는 것은 한계가 있다. (둘다 “new” 바로 뒤에 나타나기 때문에 content와 relative position 정보는 같다) “mall”이 아니라 “store”가 주어라는 정보는 absolute position을 봐야한다. 즉, 문법적 정보는 absolute position과 많은 연관이 있다.<br>DeBERTa에서는 Transformer layer와 masked token prediction을 위한 softmax layer 사이에 absolute position 정보를 준다. 이로써 DeBERTa는 transformer layer에서는 relative position 정보를 사용하고 decoder에서 mask token을 예측할 때만 absolute position을 사용한다. 실험적으로 EMD가 성능이 더 좋음을 확인하였으며, 논문에서는 BERT처럼 absolute position을 앞부분(input layer)에 넣어주는 것이 모델이 relative position 정보를 학습하는 것을 방해할 수 있다고 해석하였다.  </p>
<h3 id="3-Scale-invariant-fine-tuning"><a href="#3-Scale-invariant-fine-tuning" class="headerlink" title="3. Scale invariant fine-tuning"></a>3. Scale invariant fine-tuning</h3><h2 id="성능"><a href="#성능" class="headerlink" title="성능"></a>성능</h2><p>RoBERTa-large와 비교하여 절반의 학습 데이터로도 NLP task에서 더 좋은 성능을 냈다. 구체적으로는 MNLI에서 0.9%의 성능 향상, SQuAD v2.0에서 2.3%, RACE에서 3.6%의 성능 향상이 있었다. large 버전(transform layer 48개 - 15억개 파라미터)은 SuperGLUE에서 human performance를 능가했고 앙상블 DeBERTa 모델은 SOTA이다.(2021 1/6 기준 90.3, human performance 89.8) ((추가정리 필요)) </p>
<p><b id=footnote1>1</b>: “disentangle”의 의미를 알아보기 위해 <a target="_blank" rel="noopener" href="https://deepai.org/machine-learning-glossary-and-terms/disentangled-representation-learning">disentangled representation</a>의 설명을 찾아보았다. 각 feature을 분리된 차원으로 표현하는 것을 의미한다.<br><b id=footnote1>2</b>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">Transformer</a>, <a target="_blank" rel="noopener" href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">GPT-2</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1810.04805">Bert</a><br><b id=footnote1>3</b>: <a target="_blank" rel="noopener" href="https://openreview.net/pdf?id=rJe4ShAcF7">Music Transformer</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1906.08237">XLNet</a><br><b id=footnote1>4</b>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1901.02860">Transformer-xl</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1803.02155">Self-Attention with Relative Position Representations</a><br><b id=footnote1>5</b>: Self-Attention with Relative Position Representations이나 Music Transformer를 참고해야할 것 같다.  </p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/10/20/2020-10-20-algo5/" rel="prev" title="[알고리즘] 2020 카카오 신입공채 풀어보기 - 문제 5">
      <i class="fa fa-chevron-left"></i> [알고리즘] 2020 카카오 신입공채 풀어보기 - 문제 5
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/02/14/2021-02-14-book1/" rel="next" title="[책리뷰] 알랭드 보통 - 왜 나는 너를 사랑하는가">
      [책리뷰] 알랭드 보통 - 왜 나는 너를 사랑하는가 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%EA%B5%AC%EC%A1%B0"><span class="nav-number">1.</span> <span class="nav-text">구조</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Disentangled1-attention"><span class="nav-number">1.1.</span> <span class="nav-text">1. Disentangled1 attention</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Enhanced-masked-decoder-EMD"><span class="nav-number">1.2.</span> <span class="nav-text">2. Enhanced masked decoder(EMD)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Scale-invariant-fine-tuning"><span class="nav-number">1.3.</span> <span class="nav-text">3. Scale invariant fine-tuning</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%EC%84%B1%EB%8A%A5"><span class="nav-number">2.</span> <span class="nav-text">성능</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Yewon Park</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">7</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">categories</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yewon Park</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
